\section{Appendix}

\subsection{Parameter Calculation Verification}
\label{appendix:param-verification}

To verify that our parameter calculation formula is correct, we provide the following verification code that compares the actual parameter count from a PyTorch model (implemented in \texttt{cs336\_basics/model.py}) with our analytical formula:

\begin{lstlisting}[language=Python, caption={Parameter count verification code}]
from cs336_basics.model import TransformerLM

model_cfg = dict(
    vocab_size=10000,
    context_length=256,
    d_model=512,
    num_layers=4,
    num_heads=16,
    d_ff=1344, 
    rope_theta=10000,
)

model = TransformerLM(**model_cfg)
total_params = sum(p.numel() for p in model.parameters())

def compute_model_size_from_scratch(
    vocab_size: int,
    context_length: int,
    d_model: int,
    num_layers: int,
    num_heads: int,
    d_ff: int,
    **kwargs
):
    total_params = 0
    total_params += vocab_size * d_model  # token embedding
    total_params += num_layers * ( # per block
        4 * (d_model * d_model) +  # Q, K, V, O projections
        3 * (d_model * d_ff) +     # SwiGLU layers
        2 * (d_model)              # RMSNorm layers
    )
    total_params += d_model  # final RMSNorm
    total_params += d_model * vocab_size  # output projection
    return total_params

print(f"Total number of parameters: {total_params}")     
print("Computed model size from scratch:", compute_model_size_from_scratch(**model_cfg))
\end{lstlisting}

\begin{lstlisting}
Total number of parameters: 22696448
Computed model size from scratch: 22696448
\end{lstlisting}

\subsection{Memory Calculation Verification}
\label{appendix:memory-verification}

\begin{lstlisting}
def compute_model_size(
    vocab_size: int,
    context_length: int,
    d_model: int,
    num_layers: int,
    num_heads: int,
    d_ff: int,
    **kwargs
):
    total_params = 0
    total_params += vocab_size * d_model  # token embedding
    total_params += num_layers * ( # per block
        4 * (d_model * d_model) +  # Q, K, V, O projections
        3 * (d_model * d_ff) +     # SwiGLU layers
        2 * (d_model)              # RMSNorm layers
    )
    total_params += d_model  # final RMSNorm
    total_params += d_model * vocab_size  # output projection
    return total_params

def compute_memory(vocab_size: int,
    context_length: int,
    d_model: int,
    num_layers: int,
    num_heads: int,
    d_ff: int,
    batch_size: int,
    **kwargs
) -> float:
    param_size = compute_model_size(vocab_size, context_length, d_model, num_layers, num_heads, d_ff)
    optimizer_size = param_size * 3  # Adam optimizer
    activation_size = batch_size * context_length *(
        num_layers * (
            8 * d_model + 2 * d_ff + 2 * num_heads * context_length
        ) + d_model + 2 * vocab_size  
    )
    memory_bytes = (param_size + optimizer_size + activation_size) * 4  # assuming 4 bytes per parameter (float32)
    return memory_bytes / (1024 ** 3)  # convert to GB


if __name__ == "__main__":
    # GPT-2 XL
    cfg = dict(
        vocab_size=50257,
        context_length=1024,
        d_model=1600,
        num_layers=48,
        num_heads=25,
        d_ff=6400, 
        rope_theta=10000,
        batch_size=3.34,
    )
    model_size = compute_model_size(**cfg)
    memory_gb = compute_memory(**cfg)
    print(f"Model size: {model_size/(1e9):.2f}B parameters")
    print(f"Estimated memory usage during training: {memory_gb:.2f} GB")
\end{lstlisting}

\begin{lstlisting}
Model size: 2.13B parameters
Estimated memory usage during training: 79.97 GB
\end{lstlisting}

\subsection{GPT-2 FLOP Breakdown Code}
\label{appendix:gpt2-flop-breakdown}

To verify our FLOP breakdown for GPT-2 models, we provide the following code that computes the FLOP counts for attention, feed-forward, and language modeling head components based on the GPT-2 architecture:

\begin{lstlisting}
def compute_gpt2_flops(d_model:int, n_layer:int, T:int=1024) -> dict:
    d_ff = 4 * d_model
    vocab_size = 50_257
    attn_flops = 4 * 2 * T * d_model * d_model + 2 * 2 * T * T * d_model
    ff_flops = 3 * 2 * T * d_model * d_ff
    lm_head_flops = 2 * T * d_model * vocab_size
    total_flops = n_layer * (attn_flops + ff_flops) + lm_head_flops
    return {
        "attention": n_layer * attn_flops / total_flops,
        "feed-forward": n_layer * ff_flops / total_flops,
        "lm-head": lm_head_flops / total_flops,
        "total": total_flops / 1e12, # in TFLOPs
    }, {
        "attention": n_layer * attn_flops / 1e12,
        "feed-forward": n_layer * ff_flops / 1e12,
        "lm-head": lm_head_flops / 1e12,
        "total": total_flops / 1e12,
    }

print("GPT-2 small", compute_gpt2_flops(768, 12))
print("GPT-2 medium", compute_gpt2_flops(1024, 24))
print("GPT-2 large", compute_gpt2_flops(1280, 36))
print("GPT-2 XL", compute_gpt2_flops(1600, 48))
print("GPT-2 XL", compute_gpt2_flops(1600, 48, 16384))
\end{lstlisting}

\begin{lstlisting}
GPT-2 small ({'attention': 0.276396942718713, 'feed-forward': 0.49751449689368343, 'lm-head': 0.22608856038760353, 'total': 0.349630365696}, {'attention': 0.09663676416, 'feed-forward': 0.173946175488, 'lm-head': 0.079047426048, 'total': 0.349630365696})
GPT-2 medium ({'attention': 0.29932707434661254, 'feed-forward': 0.5986541486932251, 'lm-head': 0.1020187769601624, 'total': 1.033109504}, {'attention': 0.309237645312, 'feed-forward': 0.618475290624, 'lm-head': 0.105396568064, 'total': 1.033109504})
GPT-2 large ({'attention': 0.2996151010432329, 'feed-forward': 0.6420323593783562, 'lm-head': 0.05835253957841083, 'total': 2.2577545216}, {'attention': 0.67645734912, 'feed-forward': 1.4495514624, 'lm-head': 0.13174571008, 'total': 2.2577545216})
GPT-2 XL ({'attention': 0.29440647731422626, 'feed-forward': 0.6691056302596051, 'lm-head': 0.036487892426168594, 'total': 4.5133365248}, {'attention': 1.3287555072, 'feed-forward': 3.01989888, 'lm-head': 0.1646821376, 'total': 4.5133365248})    
GPT-2 XL ({'attention': 0.65922723665908, 'feed-forward': 0.32315060620543135, 'lm-head': 0.017622157135488675, 'total': 149.5227957248}, {'attention': 98.5694994432, 'feed-forward': 48.31838208, 'lm-head': 2.6349142016, 'total': 149.5227957248})
\end{lstlisting}

\subsection{Comparison of GPT-2 Parameter Counts with original paper}
\label{appendix:gpt2-param-comparison}

To verify our GPT-2 parameter count calculations, we provide the following comparison with the original GPT-2 paper~\cite{radford2019language}:

\begin{table}[h]
    \centering
    \caption{Comparison of our GPT-2 calculations with values reported in the original GPT-2 paper~\cite{radford2019language}.}
    \label{tab:gpt2-param-comparison}
    \begin{tabular}{c|c|c|c|c}
        \toprule
        Model & Our Calculation & Paper Reported & Difference & Relative Error \\
        \midrule
        GPT-2 Small & 124M & 117M & +7M & 5.98\% \\
        GPT-2 Medium & 354M & 345M & +9M & 2.61\% \\
        GPT-2 Large & 772M & 762M & +10M & 1.31\% \\
        GPT-2 XL & 1555M & 1542M & +13M & 0.84\% \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{lstlisting}
def compute_gpt2_params(d_model:int, num_layers:int) -> float:
    vocab_size = 50_257
    d_ff = 4 * d_model
    total_params = 0
    total_params += vocab_size * d_model  # token embedding
    total_params += num_layers * ( # per block
        4 * (d_model * d_model) +  # Q, K, V, O projections
        # 3 * (d_model * d_ff) +     # our SwiGLU layers
        2 * (d_model * d_ff) +     # original gpt-2 layers
        2 * (d_model)              # RMSNorm layers
    )
    total_params += d_model  # final RMSNorm
    # total_params += d_model * vocab_size  # output projection tied with input embedding
    return total_params


print("GPT-2 small", compute_gpt2_params(768, 12) / 1e6, "M params")
print("GPT-2 medium", compute_gpt2_params(1024, 24) / 1e6, "M params")
print("GPT-2 large", compute_gpt2_params(1280, 36) / 1e6, "M params")
print("GPT-2 XL", compute_gpt2_params(1600, 48) / 1e9, "B params")
\end{lstlisting}

\begin{lstlisting}
GPT-2 small 123.551232 M params
GPT-2 medium 353.503232 M params
GPT-2 large 772.2112 M params
GPT-2 XL 1.5551264 B params
\end{lstlisting}

\subsection{Comprehensive Logging Code Snippet}
\label{appendix:logging-code}

\begin{lstlisting}
# Training script with comprehensive logging infrastructure
import hydra
from cs336_basics.logger import Logger
from cs336_basics.config import TrainConfig

@hydra.main(config_path="conf", config_name="train_config", version_base=None)
def main(cfg: TrainConfig) -> None:
    # Initialize logger with Hydra configuration
    logger = Logger(cfg)
    output_dir = Path(HydraConfig.get().runtime.output_dir)
    
    # ... model and data setup ...
    
    # Training loop with comprehensive logging
    for it in tqdm(range(start_iter, cfg.training.max_iters), desc="Training"):
        # ... forward/backward pass ...
        
        # Training metrics logging
        if it % cfg.training.log_interval == 0:
            ent = compute_entropy_chunked(logits).mean()
            logger.log_metrics({
                'train/loss': loss.item(), 
                'train/ppl': loss.exp().item(),
                'train/lr': lr,
                'train/entropy': ent.item(),
                'train/grad_norm': grad_norm
            }, step=it)
            
        # Validation logging
        if it % cfg.training.eval_interval == 0:
            metrics = evaluate(model, val_data, cfg, device)
            logger.log_metrics({
                'val/loss': metrics['val/loss'],
                'val/ppl': metrics['val/ppl'], 
                'val/entropy': metrics['val/entropy']
            }, step=it)
    
    # Log generated text samples
    generated_output = generate_text(model, tokenizer, ...)
    logger.log_text("Generated Text", generated_output, step=cfg.training.max_iters)
    
    logger.close()
    # Save final configuration
    OmegaConf.save(cfg, output_dir / 'config.yaml')

@torch.no_grad()
def evaluate(model, data, cfg, device):
    """Evaluation with entropy and perplexity tracking."""
    # ... evaluation loop ...
    return {
        'val/loss': mean_loss,
        'val/ppl': np.exp(mean_loss),
        'val/entropy': np.mean(entropies)
    }
\end{lstlisting}

\subsection{Analysis of the Error encodings 鈥檚}
\label{appendix:error-encoding}

To investigate the source of the garbled characters like \lstinline{鈥檚} in the generated text, I analyzed the raw training data (\lstinline{owt_train.txt}), the tokenizer's behavior, and the processed binary data (\lstinline{train.bin}). My goal was to understand the frequency and representation of different apostrophe styles.

The Python snippet below confirms the root cause: the sequence \lstinline{鈥檚} is the result of incorrectly decoding a UTF-8 encoded string \lstinline{’s} (using a right single quotation mark) with a different encoding, such as GBK.

\begin{lstlisting}
>>> "NBA’s".encode("utf-8").decode("utf-8")
'NBA’s'
>>> "NBA’s".encode("utf-8").decode("gbk")
'NBA鈥檚'
\end{lstlisting}

To quantify the presence of this error in the dataset, I ran scripts to count the occurrences of different \lstinline{'s} variants both in the raw text file and in the tokenized training data\footnote{The code for this analysis can be found in \href{https://github.com/donglinkang2021/cs336-assignment1-basics/blob/main/data_utils/count_word_from_train_txt.sh}{\lstinline{count_word_from_train_txt.sh}} and \href{https://github.com/donglinkang2021/cs336-assignment1-basics/blob/main/data_utils/count_word_from_train_bin.py}{\lstinline{count_word_from_train_bin.py}}.}.

\begin{lstlisting}
(cs336-basics) [root:assignment1-basics]$ python data_utils/count_word_from_train_bin.py 
============================================================
Token Sequence Occurrence Counting Tool
============================================================
Loading tokenizer: hf_tokenizer/openwebtext-32k/tokenizer.json
'鈥檚' -> token_ids: [164, 187, 99, 161, 104, 205]
''s' -> token_ids: [382]
'’s' -> token_ids: [286, 83]
'‘s' -> token_ids: [4719, 83]
------------------------------------------------------------
File size: 5.08 GB
Total tokens: 2,727,044,684
Target token sequences:
  '鈥檚': [164, 187, 99, 161, 104, 205]
  ''s': [382]
  '’s': [286, 83]
  '‘s': [4719, 83]
Start counting...
Counting progress: 100%|██████████████████████████████████| 2727044684/2727044684 [03:48<00:00, 11959960.05tokens/s]

Counting results:
----------------------------------------
'鈥檚' (sequence: [164, 187, 99, 161, 104, 205]): 2 times
''s' (sequence: [382]): 8,062,975 times
'’s' (sequence: [286, 83]): 11,253,531 times
'‘s' (sequence: [4719, 83]): 24,540 times

Total tokens: 2,727,044,684
Processing time: 228.02 seconds
============================================================
(cs336-basics) [root:assignment1-basics]$ bash data_utils/count_word_from_train_txt.sh 
Counting character occurrences in data/owt_train.txt...
File size: 12G
Occurrences of '鈥檚'
2
Occurrences of ''s'
8131046
Occurrences of '’s'
11277977
Occurrences of '‘s'
71423

Counting completed!
\end{lstlisting}

\textbf{Some Findings:}

The results from both the raw text and the tokenized data are consistent. They reveal that the dataset is dominated by standard apostrophes, with \lstinline{’s} (right single quote) being the most frequent variant (over 11 million times), followed by \lstinline{'s} (straight quote, over 8 million times).

Crucially, the garbled sequence \lstinline{鈥檚} appears only twice in the entire 12GB dataset. This confirms that the issue is not widespread data corruption but rather a rare artifact, likely from the original web scraping process.

The paradox that the model reproduces this extremely rare pattern invites some interesting speculation. Here are a couple of my hypotheses:

\textbf{1. Sensitivity to Low-Probability Patterns:} This behavior might indicate that the model is highly sensitive to every pattern it has learned, even those with vanishingly low probabilities. During a degenerative state where the model loses long-range context, it might latch onto any learned sequence that fits the immediate context. The fact that it can recall and repeat a pattern seen only twice suggests that no training example is truly forgotten, merely assigned a low probability.

\textbf{2. Preference for Longer Token Sequences:} An alternative hypothesis is that the model may develop a preference for longer token sequences to fulfill certain grammatical roles. The garbled \lstinline{鈥檚} is tokenized into a sequence of six tokens, whereas the correct \lstinline{'s} and \lstinline{’s} are tokenized into one and two tokens, respectively. It's conceivable that when the model is struggling to form a possessive, it defaults to a longer, more "descriptive" token sequence it has learned, even if it's incorrect. This is a fascinating thought, as it might offer a glimpse into why techniques like Chain-of-Thought (CoT) are effective—they guide the model to solve problems through longer, more deliberate sequences of tokens.
