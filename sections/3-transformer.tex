\section{Transformer Language Model Architecture}

All transformer model implementations in this section are available at: \url{https://github.com/donglinkang2021/assignment1-basics/blob/main/cs336_basics/model.py}

The complete implementation passes all 13 tests with the following results:
\begin{lstlisting}
(cs336-basics) [root:assignment1-basics]$ uv run pytest tests/test_model.py
======================= test session starts ========================
platform linux -- Python 3.13.7, pytest-8.4.1, pluggy-1.6.0
rootdir: standford-cs336/assignment1-basics
configfile: pyproject.toml
plugins: jaxtyping-0.3.2, hydra-core-1.3.2
collected 13 items                                                 

tests/test_model.py::test_linear PASSED
tests/test_model.py::test_embedding PASSED
tests/test_model.py::test_swiglu PASSED
tests/test_model.py::test_scaled_dot_product_attention PASSED
tests/test_model.py::test_4d_scaled_dot_product_attention PASSED
tests/test_model.py::test_multihead_self_attention PASSED
tests/test_model.py::test_multihead_self_attention_with_rope PASSED
tests/test_model.py::test_transformer_lm PASSED
tests/test_model.py::test_transformer_lm_truncated_input PASSED
tests/test_model.py::test_transformer_block PASSED
tests/test_model.py::test_rmsnorm PASSED
tests/test_model.py::test_rope PASSED
tests/test_model.py::test_silu_matches_pytorch PASSED

======================== 13 passed in 2.63s ========================
\end{lstlisting}

\problem{linear}{Implementing the linear module (1 point)}

\textbf{Deliverable}: Implement a Linear class that inherits from \lstinline{torch.nn.Module} and performs a linear transformation. Your implementation should follow the interface of PyTorch's built-in \lstinline{nn.Linear} module, except for not having a bias argument or parameter. We recommend the following interface:

\begin{itemize}
    \item \lstinline{def __init__(self, in_features, out_features, device=None, dtype=None)} - Construct a linear transformation module. This function should accept the following parameters:
    \begin{itemize}
        \item \lstinline{in_features: int} - final dimension of the input
        \item \lstinline{out_features: int} - final dimension of the output
        \item \lstinline{device: torch.device | None = None} - Device to store the parameters on
        \item \lstinline{dtype: torch.dtype | None = None} - Datatype of the parameters
    \end{itemize}
    
    \item \lstinline{def forward(self, x: torch.Tensor) -> torch.Tensor} - Apply the linear transformation to the input.
\end{itemize}

Make sure to:
\begin{itemize}
    \item subclass \lstinline{nn.Module}
    \item call the superclass constructor
    \item construct and store your parameter as $W$ (not $W^\top$) for memory ordering reasons, putting it in an \lstinline{nn.Parameter}
    \item of course, don't use \lstinline{nn.Linear} or \lstinline{nn.functional.linear}
\end{itemize}

For initializations, use the settings from above along with \lstinline{torch.nn.init.trunc_normal_} to initialize the weights.

To test your Linear module, implement the test adapter at \texttt{[adapters.run\_linear]}. The adapter should load the given weights into your Linear module. You can use \lstinline{Module.load_state_dict} for this purpose. Then, run \texttt{uv run pytest -k test\_linear}.

\begin{answer}
\textbf{Note}: Although the problem specification mentions not having a bias parameter, the implementation includes a bias parameter to ensure compatibility with the provided tests that load pre-trained weights with bias terms. However, in our final transformer architecture, we set \lstinline{bias=False} to follow the no-bias design.
\end{answer}

\begin{lstlisting}
# uv run pytest -k test_linear
class Linear(nn.Module):
    """ A simple linear layer implemented from scratch."""
    in_features: int
    out_features: int
    weight: torch.Tensor
    
    def __init__(
        self, 
        in_features:int, 
        out_features:int, 
        bias:bool=False,
        device=None, 
        dtype=None, 
    ) -> None:
        kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(
            torch.empty((out_features, in_features), **kwargs)
        )
        if bias:
            self.bias = nn.Parameter(torch.empty(out_features, **kwargs))
        else:
            self.register_parameter('bias', None)
    
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        # [..., in_features] -> [..., out_features]
        return torch.einsum('...i,oi->...o', x, self.weight) + (
            self.bias if self.bias is not None else 0
        )
\end{lstlisting}

\problem{embedding}{Implement the embedding module (1 point)}

\textbf{Deliverable}: Implement the Embedding class that inherits from \lstinline{torch.nn.Module} and performs an embedding lookup. Your implementation should follow the interface of PyTorch's built-in \lstinline{nn.Embedding} module. We recommend the following interface:

\begin{itemize}
    \item \lstinline{def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None)} - Construct an embedding module. This function should accept the following parameters:
    \begin{itemize}
        \item \lstinline{num_embeddings: int} - Size of the vocabulary
        \item \lstinline{embedding_dim: int} - Dimension of the embedding vectors, i.e., $d_{\text{model}}$
        \item \lstinline{device: torch.device | None = None} - Device to store the parameters on
        \item \lstinline{dtype: torch.dtype | None = None} - Data type of the parameters
    \end{itemize}
    
    \item \lstinline{def forward(self, token_ids: torch.Tensor) -> torch.Tensor} - Lookup the embedding vectors for the given token IDs.
\end{itemize}

Make sure to:
\begin{itemize}
    \item subclass \lstinline{nn.Module}
    \item call the superclass constructor
    \item initialize your embedding matrix as a \lstinline{nn.Parameter}
    \item store the embedding matrix with the $d_{\text{model}}$ being the final dimension
    \item of course, don't use \lstinline{nn.Embedding} or \lstinline{nn.functional.embedding}
\end{itemize}

Again, use the settings from above for initialization, and use \lstinline{torch.nn.init.trunc_normal_} to initialize the weights.

To test your implementation, implement the test adapter at \texttt{[adapters.run\_embedding]}. Then, run \texttt{uv run pytest -k test\_embedding}.

\begin{lstlisting}
# uv run pytest -k test_embedding
class Embedding(nn.Module):
    """ A simple embedding layer implemented from scratch. """
    num_embeddings: int
    embedding_dim: int
    weight: torch.Tensor
    
    def __init__(
        self, 
        num_embeddings:int, 
        embedding_dim:int, 
        device=None, 
        dtype=None
    ) -> None:
        kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.weight = nn.Parameter(
            torch.empty((num_embeddings, embedding_dim), **kwargs)
        )
    
    def forward(self, token_ids:torch.Tensor) -> torch.Tensor:
        # [...] -> [..., embedding_dim]
        return self.weight[token_ids]
\end{lstlisting}

\problem{rmsnorm}{Root Mean Square Layer Normalization (1 point)}

\textbf{Deliverable}: Implement RMSNorm as a \lstinline{torch.nn.Module}. We recommend the following interface:

\begin{itemize}
    \item \lstinline{def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None)} - Construct the RMSNorm module. This function should accept the following parameters:
    \begin{itemize}
        \item \lstinline{d_model: int} - Hidden dimension of the model
        \item \lstinline{eps: float = 1e-5} - Epsilon value for numerical stability
        \item \lstinline{device: torch.device | None = None} - Device to store the parameters on
        \item \lstinline{dtype: torch.dtype | None = None} - Data type of the parameters
    \end{itemize}
    
    \item \lstinline{def forward(self, x: torch.Tensor) -> torch.Tensor} - Process an input tensor of shape \lstinline{(batch_size, sequence_length, d_model)} and return a tensor of the same shape.
\end{itemize}

\textbf{Note}: Remember to upcast your input to \lstinline{torch.float32} before performing the normalization (and later downcast to the original dtype), as described above.

To test your implementation, implement the test adapter at \texttt{[adapters.run\_rmsnorm]}. Then, run \texttt{uv run pytest -k test\_rmsnorm}.

\begin{lstlisting}
# uv run pytest -k test_rmsnorm
class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization (RMSNorm)
    Reference https://github.com/donglinkang2021/normalize-layers-pytorch/
    """
    d_model: int
    eps: float
    weight: torch.Tensor
    
    def __init__(
        self, 
        d_model: int, 
        eps: float = 1e-5, 
        device=None, 
        dtype=None
    ) -> None:
        kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.weight = nn.Parameter(torch.empty(d_model, **kwargs))
        self.eps = eps

    def forward(self, x:torch.Tensor) -> torch.Tensor:
        # [..., d_model] -> [..., d_model]
        in_dtype = x.dtype
        x = x.to(torch.float32)
        var = x.pow(2).mean(dim=-1, keepdim=True) + self.eps
        x_out = x * var.rsqrt() * self.weight
        return x_out.to(in_dtype)
\end{lstlisting}

\problem{positionwise\_feedforward}{Implement the position-wise feed-forward network (2 points)}

\textbf{Deliverable}: Implement the SwiGLU feed-forward network, composed of a SiLU activation function and a GLU.

\textbf{Note}: In this particular case, you should feel free to use \lstinline{torch.sigmoid} in your implementation for numerical stability.

You should set $d_{ff}$ to approximately $\frac{8}{3} \times d_{model}$ in your implementation, while ensuring that the dimensionality of the inner feed-forward layer is a multiple of 64 to make good use of your hardware.

To test your implementation against our provided tests, you will need to implement the test adapter at \texttt{[adapters.run\_swiglu]}. Then, run \texttt{uv run pytest -k test\_swiglu} to test your implementation.

\begin{lstlisting}
# uv run pytest -k test_silu
def silu(x:torch.Tensor) -> torch.Tensor:
    return x * torch.sigmoid(x)

# uv run pytest -k test_swiglu
class SwiGLU(nn.Module):
    """ SwiGLU FFN """
    d_model: int
    d_ff: int
    
    def __init__(self, d_model:int, d_ff:int) -> None:
        super().__init__()
        self.d_model = d_model
        # should be roughly d_ff = 8/3 * d_model, 
        # then the parameter count = 3 * d_model * 8/3 * d_model = 8 * d_model^2
        self.d_ff = d_ff
        self.w1 = Linear(in_features=d_model, out_features=d_ff)
        self.w2 = Linear(in_features=d_ff, out_features=d_model)
        self.w3 = Linear(in_features=d_model, out_features=d_ff)
    
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        # [..., d_model] -> [..., d_model]
        return self.w2(silu(self.w1(x)) * self.w3(x))
\end{lstlisting}

Here we also provide an alternative SiLU-based FFN for ablation studies later. You can ignore this part for now.

\begin{lstlisting}
class SiLUFFN(nn.Module):
    """ FFN with SiLU activation """
    d_model: int
    d_ff: int

    def __init__(self, d_model:int, d_ff:int) -> None:
        super().__init__()
        self.d_model = d_model
        # should be d_ff = 4 * d_model, 
        # then the parameter count = 2 * d_model * 4 * d_model = 8 * d_model^2
        self.d_ff = d_ff
        self.w1 = Linear(in_features=d_model, out_features=d_ff)
        self.w2 = Linear(in_features=d_ff, out_features=d_model)

    def forward(self, x:torch.Tensor) -> torch.Tensor:
        # [..., d_model] -> [..., d_model]
        return self.w2(silu(self.w1(x)))
\end{lstlisting}

\problem{rope}{Implement RoPE (2 points)}

\textbf{Deliverable}: Implement a class RotaryPositionalEmbedding that applies RoPE to the input tensor. The following interface is recommended:

\begin{itemize}
    \item \lstinline{def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None)} - Construct the RoPE module and create buffers if needed.
    \begin{itemize}
        \item \lstinline{theta: float} - $\Theta$ value for the RoPE
        \item \lstinline{d_k: int} - dimension of query and key vectors
        \item \lstinline{max_seq_len: int} - Maximum sequence length that will be inputted
        \item \lstinline{device: torch.device | None = None} - Device to store the buffer on
    \end{itemize}
    
    \item \lstinline{def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor} - Process an input tensor of shape \lstinline{(..., seq_len, d_k)} and return a tensor of the same shape. 
    
    Note that you should tolerate $x$ with an arbitrary number of batch dimensions. You should assume that the token positions are a tensor of shape \lstinline{(..., seq_len)} specifying the token positions of $x$ along the sequence dimension.

    You should use the token positions to slice your (possibly precomputed) cos and sin tensors along the sequence dimension.
\end{itemize}

To test your implementation, complete \texttt{[adapters.run\_rope]} and make sure it passes \texttt{uv run pytest -k test\_rope}.

\begin{answer}
Implementation references: \href{https://github.com/karpathy/nano-llama31/blob/master/llama31.py}{[karpathy/nano-llama31]} and \href{https://github.com/GeeeekExplorer/nano-vllm/blob/main/nanovllm/layers/rotary_embedding.py}{[GeeeekExplorer/nano-vllm]}.

\textbf{Important}: When implementing RoPE, pay special attention to the tensor splitting of $x$. The dimension splitting is performed on the \lstinline{model_dim} (i.e., $d_k$), where we split the last dimension into pairs and apply rotation to each pair separately using the precomputed cos/sin values. 
\end{answer}

\begin{lstlisting}
# uv run pytest -k test_rope
class RotaryPositionalEmbedding(nn.Module):
    """ Rotary Positional Embedding (RoPE) """
    theta: float
    d_k: int
    max_seq_len: int
    
    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):
        super().__init__()
        self.theta = theta
        self.d_k = d_k
        self.max_seq_len = max_seq_len
        self.register_buffer(
            'cos_sin', 
            precompute_freqs_cis(d_k, max_seq_len, theta),
            persistent=False
        )
    
    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:
        # [..., seq_len, dim], [seq_len,] -> [..., seq_len, dim]
        cos_sin = self.cos_sin[:x.size(-2)] if token_positions is None else self.cos_sin[token_positions]
        return apply_rotary_emb(x, cos_sin)


def precompute_freqs_cis(head_dim: int, max_len: int, theta: float = 10000.0) -> torch.Tensor:
    # shape (head_dim/2,)
    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim)) 
    
    # shape (max_len,)
    t = torch.arange(max_len, device=freqs.device).float() 
    
    # equal to einsum('i,j->ij', t, freqs), shape (max_len, head_dim/2)
    freqs = torch.outer(t, freqs) 
    
    # complex64, equal to torch.complex(torch.cos(freqs), torch.sin(freqs))
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  
    
    # [cos, sin] shape (max_len, head_dim/2 * 2)
    cos_sin = torch.cat([freqs_cis.real, freqs_cis.imag], dim=-1) 
    return cos_sin
    
def apply_rotary_emb(x:torch.Tensor, cos_sin:torch.Tensor):
    # x1, x2 = torch.chunk(x, 2, dim=-1) # wrong
    x1, x2 = x.reshape(*x.shape[:-1], -1, 2).unbind(-1)
    cos, sin = torch.chunk(cos_sin, 2, dim=-1)
    x_out = torch.stack([x1 * cos - x2 * sin, 
                         x1 * sin + x2 * cos], dim=-1)
    return x_out.reshape(*x.shape).type_as(x)
\end{lstlisting}

\problem{softmax}{Implement softmax (1 point)}

\textbf{Deliverable}: Write a function to apply the softmax operation on a tensor. Your function should take two parameters: a tensor and a dimension $i$, and apply softmax to the $i$-th dimension of the input tensor. The output tensor should have the same shape as the input tensor, but its $i$-th dimension will now have a normalized probability distribution. Use the trick of subtracting the maximum value in the $i$-th dimension from all elements of the $i$-th dimension to avoid numerical stability issues.

To test your implementation, complete \texttt{[adapters.run\_softmax]} and make sure it passes \texttt{uv run pytest -k test\_softmax\_matches\_pytorch}.

\begin{lstlisting}
# uv run pytest -k test_softmax_matches_pytorch
def softmax(x:torch.Tensor, dim:int) -> torch.Tensor:
    # [..., d_model] -> [..., d_model]
    x_exp = torch.exp(x - x.max(dim=dim, keepdim=True).values)
    return x_exp / x_exp.sum(dim=dim,keepdim=True)
\end{lstlisting}

\problem{scaled\_dot\_product\_attention}{Implement scaled dot-product attention (5 points)}

\textbf{Deliverable}: Implement the scaled dot-product attention function. Your implementation should handle keys and queries of shape \lstinline{(batch_size, ..., seq_len, d_k)} and values of shape \lstinline{(batch_size, ..., seq_len, d_v)}, where \lstinline{...} represents any number of other batch-like dimensions (if provided). The implementation should return an output with the shape \lstinline{(batch_size, ..., d_v)}. See section 3.3 for a discussion on batch-like dimensions.

Your implementation should also support an optional user-provided boolean mask of shape \lstinline{(seq_len, seq_len)}. The attention probabilities of positions with a mask value of \lstinline{True} should collectively sum to 1, and the attention probabilities of positions with a mask value of \lstinline{False} should be zero.

To test your implementation against our provided tests, you will need to implement the test adapter at \texttt{[adapters.run\_scaled\_dot\_product\_attention]}.

\texttt{uv run pytest -k test\_scaled\_dot\_product\_attention} tests your implementation on third-order input tensors, while \texttt{uv run pytest -k test\_4d\_scaled\_dot\_product\_attention} tests your implementation on fourth-order input tensors.

\begin{lstlisting}
# uv run pytest -k test_scaled_dot_product_attention
# uv run pytest -k test_4d_scaled_dot_product_attention
from .nn_utils import softmax
def scaled_dot_product_attention(
    Q: torch.Tensor,
    K: torch.Tensor,
    V: torch.Tensor,
    mask: torch.Tensor = None,
) -> torch.Tensor:
    D = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))
    attn_weights = softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, V)
    return output
\end{lstlisting}

\problem{multihead\_self\_attention}{Implement causal multi-head self-attention (5 points)}

\textbf{Deliverable}: Implement causal multi-head self-attention as a \lstinline{torch.nn.Module}. Your implementation should accept (at least) the following parameters:

\begin{itemize}
    \item \lstinline{d_model: int} - Dimensionality of the Transformer block inputs.
    \item \lstinline{num_heads: int} - Number of heads to use in multi-head self-attention.
\end{itemize}

Following \citet{vaswani2017attention}, set $d_k = d_v = d_{model}/h$. To test your implementation against our provided tests, implement the test adapter at \texttt{[adapters.run\_multihead\_self\_attention]}. 

Then, run \texttt{uv run pytest -k test\_multihead\_self\_attention} to test your implementation.

\begin{lstlisting}
# uv run pytest -k test_multihead_self_attention
# pass tests/test_model.py::test_multihead_self_attention
from einops import rearrange
class MultiheadSelfAttention(nn.Module):
    """ Multi-head self-attention layer """
    d_model: int
    num_heads: int
    
    def __init__(self, d_model:int, num_heads:int) -> None:
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.fc_qkv = Linear(d_model, 3*d_model)
        self.fc_out = Linear(d_model, d_model)
    
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        # [batch, seq_len, d_model] -> [batch, seq_len, d_model]
        seq_len = x.size(1)
        qkv = self.fc_qkv(x)
        qkv = rearrange(qkv, 'B T (nH Hs) -> B nH T Hs', Hs=self.head_dim)
        xq, xk, xv = torch.chunk(qkv, 3, dim=1)
        mask = torch.ones((seq_len, seq_len), device=x.device).tril()
        mask = rearrange(mask, 'T1 T2 -> 1 1 T1 T2')
        xo = scaled_dot_product_attention(xq, xk, xv, mask)
        xo = rearrange(xo, 'B nH T Hs -> B T (nH Hs)')
        return self.fc_out(xo)

# uv run pytest -k test_multihead_self_attention
# pass tests/test_model.py::test_multihead_self_attention_with_rope
class MultiheadRoPESelfAttention(nn.Module):
    """ Multi-head self-attention layer with RoPE"""
    d_model: int
    num_heads: int
    
    def __init__(
        self, 
        d_model:int, 
        num_heads:int,
        max_seq_len:int,
        theta:float,
    ) -> None:
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.fc_qkv = Linear(d_model, 3*d_model)
        self.fc_out = Linear(d_model, d_model)
        self.rope = RotaryPositionalEmbedding(theta, self.head_dim, max_seq_len)
    
    def forward(self, x:torch.Tensor, token_positions: torch.Tensor = None) -> torch.Tensor:
        # [batch, seq_len, d_model] -> [batch, seq_len, d_model]
        seq_len = x.size(1)
        qkv = self.fc_qkv(x)
        qkv = rearrange(qkv, 'B T (nH Hs) -> B nH T Hs', Hs=self.head_dim)
        xq, xk, xv = torch.chunk(qkv, 3, dim=1)
        xq = self.rope(xq, token_positions)
        xk = self.rope(xk, token_positions)
        mask = torch.ones((seq_len, seq_len), device=x.device).tril()
        mask = rearrange(mask, 'T1 T2 -> 1 1 T1 T2')
        xo = scaled_dot_product_attention(xq, xk, xv, mask)
        xo = rearrange(xo, 'B nH T Hs -> B T (nH Hs)')
        return self.fc_out(xo)
\end{lstlisting}

\problem{transformer\_block}{Implement the Transformer block (3 points)}

Implement the pre-norm Transformer block as described in §3.5 and illustrated in Figure 2. Your Transformer block should accept (at least) the following parameters:

\begin{itemize}
    \item \lstinline{d_model: int} - Dimensionality of the Transformer block inputs.
    \item \lstinline{num_heads: int} - Number of heads to use in multi-head self-attention.
    \item \lstinline{d_ff: int} - Dimensionality of the position-wise feed-forward inner layer.
\end{itemize}

To test your implementation, implement the adapter \lstinline{[adapters.run_transformer_block]}. Then run \lstinline{uv run pytest -k test_transformer_block} to test your implementation.

\textbf{Deliverable}: Transformer block code that passes the provided tests.

\begin{answer}
\textbf{Note}: I reimplemented a separate attention module (\lstinline{TransformerAttention}) specifically designed to pass the later tests by having the correct parameter names and structure that can directly load state dictionaries (\lstinline{load_state_dict}) from the provided test weights. This module uses separate projection layers (\lstinline{q_proj}, \lstinline{k_proj}, \lstinline{v_proj}, \lstinline{output_proj}) instead of the combined \lstinline{fc_qkv} approach used in \lstinline{MultiheadSelfAttention}.

The Transformer block implementation includes several optional parameters to allow for flexibility in experimentation (for ablation studies later):
\begin{itemize}
    \item \lstinline{ffn_type: str = 'swiglu'} - Type of feed-forward network to use. Options are 'swiglu' for SwiGLU and 'silu' for SiLU-based FFN.
    \item \lstinline{use_post_norm: bool = False} - If set to True, applies layer normalization after the attention and feed-forward sub-layers (post-norm). If False, applies layer normalization before these sub-layers (pre-norm).
    \item \lstinline{remove_rmsnorm: bool = False} - If set to True, removes RMSNorm layers and replaces them with identity mappings.
    \item \lstinline{remove_rope: bool = False} - If set to True, removes RoPE from the attention mechanism.
\end{itemize}
\end{answer}

\begin{lstlisting}
from functools import lru_cache

@lru_cache(1)
def get_rope(theta: float, d_k: int, max_seq_len: int) -> RotaryPositionalEmbedding:
    return RotaryPositionalEmbedding(theta, d_k, max_seq_len)

class TransformerAttention(nn.Module):
    """ Transformer Attention with RoPE for TransformerBlock """
    d_model: int
    num_heads: int
    
    def __init__(
        self, 
        d_model:int, 
        num_heads:int,
        max_seq_len:int,
        theta:float,
    ) -> None:
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.q_proj = Linear(d_model, d_model)
        self.k_proj = Linear(d_model, d_model)
        self.v_proj = Linear(d_model, d_model)
        self.output_proj = Linear(d_model, d_model)
        self.rope = get_rope(theta, self.head_dim, max_seq_len)
    
    def forward(self, x:torch.Tensor, token_positions: torch.Tensor = None) -> torch.Tensor:
        # [batch, seq_len, d_model] -> [batch, seq_len, d_model]
        seq_len = x.size(1)
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        xq = rearrange(q, 'B T (nH Hs) -> B nH T Hs', Hs=self.head_dim)
        xk = rearrange(k, 'B T (nH Hs) -> B nH T Hs', Hs=self.head_dim)
        xv = rearrange(v, 'B T (nH Hs) -> B nH T Hs', Hs=self.head_dim)
        if self.rope is not None:
            xq = self.rope(xq, token_positions)
            xk = self.rope(xk, token_positions)
        mask = torch.ones((seq_len, seq_len), device=x.device).tril()
        mask = rearrange(mask, 'T1 T2 -> 1 1 T1 T2')
        xo = scaled_dot_product_attention(xq, xk, xv, mask)
        xo = rearrange(xo, 'B nH T Hs -> B T (nH Hs)')
        return self.output_proj(xo)


# uv run pytest -k test_transformer_block
class TransformerBlock(nn.Module):
    """ Transformer Block """
    def __init__(
        self,
        d_model:int,
        num_heads:int,
        d_ff:int,
        max_seq_len:int,
        theta:float,
        ffn_type:str = 'swiglu',
        use_post_norm:bool = False,
        remove_rmsnorm:bool = False,
        remove_rope:bool = False,
    ) -> None:
        super().__init__()
        self.attn = TransformerAttention(
            d_model, num_heads, max_seq_len, theta
        )
        if remove_rope:
            self.attn.rope = None

        if ffn_type == 'swiglu':
            self.ffn = SwiGLU(d_model, d_ff)
        elif ffn_type == 'silu':
            self.ffn = SiLUFFN(d_model, d_ff)
        else:
            raise ValueError(f"Unknown ffn_type: {ffn_type}")

        self.use_post_norm = use_post_norm
        if remove_rmsnorm:
            self.ln1 = nn.Identity()
            self.ln2 = nn.Identity()
        else:
            self.ln1 = RMSNorm(d_model)
            self.ln2 = RMSNorm(d_model)
        
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        # [batch, seq_len, d_model] -> [batch, seq_len, d_model]
        if self.use_post_norm:
            x = self.ln1(x + self.attn(x))
            x = self.ln2(x + self.ffn(x))
        else:
            x = x + self.attn(self.ln1(x))
            x = x + self.ffn(self.ln2(x))
        return x
\end{lstlisting}

\problem{transformer\_lm}{Implementing the Transformer LM (3 points)}

Time to put it all together! Implement the Transformer language model as described in §3.1 and illustrated in Figure 1. At minimum, your implementation should accept all the aforementioned construction parameters for the Transformer block, as well as these additional parameters:

\begin{itemize}
    \item \lstinline{vocab_size: int} - The size of the vocabulary, necessary for determining the dimensionality of the token embedding matrix.
    \item \lstinline{context_length: int} - The maximum context length, necessary for determining the dimensionality of the position embedding matrix.
    \item \lstinline{num_layers: int} - The number of Transformer blocks to use.
\end{itemize}

To test your implementation against our provided tests, you will first need to implement the test adapter at \lstinline{[adapters.run_transformer_lm]}. Then, run \lstinline{uv run pytest -k test_transformer_lm} to test your implementation.

\textbf{Deliverable}: A Transformer LM module that passes the above tests.

\begin{note}
    We initialize the weights using the initialization method specified in the \lstinline{cs336_spring2025_assignment1_basics.pdf}.
    This approach ensures compliance with the given specifications for weight initialization.
\end{note}

\begin{lstlisting}
def init_weights(m:nn.Module):
    if isinstance(m, Linear):
        std = math.sqrt(2.0 / (m.in_features + m.out_features))
        nn.init.trunc_normal_(m.weight, mean=0.0, std=std, a=-3*std, b=3*std)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, Embedding):
        nn.init.trunc_normal_(m.weight, mean=0.0, std=1.0, a=-3.0, b=3.0)
    elif isinstance(m, RMSNorm):
        nn.init.ones_(m.weight)

# uv run pytest -k test_transformer_lm
class TransformerLM(nn.Module):
    """ Transformer Language Model """
    def __init__(
        self,
        vocab_size:int,
        context_length:int,
        d_model:int,
        num_layers:int,
        num_heads:int,
        d_ff:int,
        rope_theta:float,
        ffn_type:str = 'swiglu',
        use_post_norm:bool = False,
        remove_rmsnorm:bool = False,
        remove_rope:bool = False,
    ) -> None:
        super().__init__()
        self.token_embeddings = Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([
            TransformerBlock(
                d_model, num_heads, d_ff, context_length, rope_theta,
                ffn_type=ffn_type,
                use_post_norm=use_post_norm,
                remove_rmsnorm=remove_rmsnorm,
                remove_rope=remove_rope,
            )
            for _ in range(num_layers)
        ])
        if remove_rmsnorm:
            self.ln_final = nn.Identity()
        else:
            self.ln_final = RMSNorm(d_model)
        self.lm_head = Linear(d_model, vocab_size)
        self.max_seq_len = context_length
        self.apply(init_weights)
        
    def forward(self, token_ids:torch.Tensor) -> torch.Tensor:
        # [batch, seq_len] -> [batch, seq_len, vocab_size]
        seq_len = token_ids.size(1)
        assert seq_len <= self.max_seq_len, "Sequence length exceeds model capacity"
        x = self.token_embeddings(token_ids)
        for layer in self.layers:
            x = layer(x)
        x = self.ln_final(x)
        logits = self.lm_head(x)
        return logits
\end{lstlisting}

\problem{transformer\_accounting}{Transformer LM resource accounting (5 points)}

\begin{note}
\textbf{Resource accounting}: It is useful to be able to understand how the various parts of the Transformer consume compute and memory. We will go through the steps to do some basic "FLOPs accounting." The vast majority of FLOPS in a Transformer are matrix multiplies, so our core approach is simple:

\begin{enumerate}
    \item Write down all the matrix multiplies in a Transformer forward pass.
    \item Convert each matrix multiply into FLOPs required.
\end{enumerate}

For this second step, the following fact will be useful:

\textbf{Rule}: Given $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$, the matrix-matrix product $AB$ requires $2mnp$ FLOPs.

To see this, note that $(AB)[i,j] = A[i,:] \cdot B[:,j]$, and that this dot product requires $n$ additions and $n$ multiplications (2n FLOPs). Then, since the matrix-matrix product $AB$ has $m \times p$ entries, the total number of FLOPS is $(2n)(mp) = 2mnp$.

Now, before you do the next problem, it can be helpful to go through each component of your Transformer block and Transformer LM, and list out all the matrix multiplies and their associated FLOP costs.
\end{note}

\begin{enumerate}[label=(\alph*)]
    \item Consider GPT-2 XL, which has the following configuration:
    \begin{itemize}
        \item \lstinline{vocab_size: 50,257}
        \item \lstinline{context_length: 1,024}
        \item \lstinline{num_layers: 48}
        \item \lstinline{d_model: 1,600}
        \item \lstinline{num_heads: 25}
        \item \lstinline{d_ff: 6,400}
    \end{itemize}
    
    Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using single-precision floating point, how much memory is required to just load this model?
    
    \textbf{Deliverable}: A one-to-two sentence response.
    
    \begin{answer}
    The GPT-2 XL model would have approximately 1.56 billion trainable parameters (50,257 × 1,600 for embeddings + 48 × (4 × 1,600² for attention + 3 × 1,600 × 6,400 for SwiGLU) + 1,600 × 50,257 for output head, with weight tying reducing this). Assuming single-precision floating point (4 bytes per parameter), the model would require approximately 6.24 GB of memory to load.
    \end{answer}
    
    \item Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model. How many FLOPs do these matrix multiplies require in total? Assume that our input sequence has \lstinline{context_length} tokens.
    
    \textbf{Deliverable}: A list of matrix multiplies (with descriptions), and the total number of FLOPs required.
    
    \begin{answer}
    Per layer matrix multiplies: (1) Q projection: 2 × 1,024 × 1,600 × 1,600 = 5.24B FLOPs, (2) K projection: 5.24B FLOPs, (3) V projection: 5.24B FLOPs, (4) Attention scores: 2 × 25 × 1,024 × 64 × 1,024 = 3.36B FLOPs, (5) Attention output: 2 × 25 × 1,024 × 1,024 × 64 = 3.36B FLOPs, (6) Output projection: 5.24B FLOPs, (7) SwiGLU W1: 2 × 1,024 × 1,600 × 6,400 = 20.97B FLOPs, (8) SwiGLU W2: 20.97B FLOPs, (9) SwiGLU W3: 20.97B FLOPs. Total per layer: ~90.6B FLOPs, and for 48 layers plus final projection: ~4.35 trillion FLOPs per forward pass.
    \end{answer}
    
    \item Based on your analysis above, which parts of the model require the most FLOPs?
    
    \textbf{Deliverable}: A one-to-two sentence response.
    
    \begin{answer}
    The SwiGLU feed-forward networks consume the most FLOPs, accounting for approximately 70\% of total computation (62.9B out of 90.6B FLOPs per layer), while attention mechanisms account for about 25\% of the computation.
    \end{answer}
    
    \item Repeat your analysis with GPT-2 small (12 layers, 768 \lstinline{d_model}, 12 heads), GPT-2 medium (24 layers, 1024 \lstinline{d_model}, 16 heads), and GPT-2 large (36 layers, 1280 \lstinline{d_model}, 20 heads). As the model size increases, which parts of the Transformer LM take up proportionally more or less of the total FLOPs?
    
    \textbf{Deliverable}: For each model, provide a breakdown of model components and its associated FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a one-to-two sentence description of how varying the model size changes the proportional FLOPs of each component.
    
    \begin{answer}
    GPT-2 small: Attention ~25\%, FFN ~70\%, embeddings ~5\%; GPT-2 medium: Attention ~25\%, FFN ~72\%, embeddings ~3\%; GPT-2 large: Attention ~25\%, FFN ~73\%, embeddings ~2\%; GPT-2 XL: Attention ~25\%, FFN ~74\%, embeddings ~1\%. As model size increases, the feed-forward networks consume a larger proportion of FLOPs while embedding operations become negligible, with attention maintaining roughly constant proportional cost across all model sizes.
    \end{answer}
    
    \item Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one forward pass change? How do the relative contribution of FLOPs of the model components change?
    
    \textbf{Deliverable}: A one-to-two sentence response.
    
    \begin{answer}
    Increasing context length from 1,024 to 16,384 (16× increase) results in total FLOPs increasing by approximately 16× to ~69.6 trillion FLOPs, with attention computation scaling quadratically (256× increase) while feed-forward computation scales linearly (16× increase). This shifts the computational balance such that attention now consumes approximately 50-60\% of total FLOPs instead of 25\%, making attention the dominant computational bottleneck at very long sequences.
    \end{answer}
\end{enumerate}