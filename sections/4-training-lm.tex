\section{Training a Transformer LM}

\problem{cross\_entropy}{Implement Cross entropy (2 points)}

\textbf{Deliverable}: Write a function to compute the cross entropy loss, which takes in predicted logits ($o_i$) and targets ($x_{i+1}$) and computes the cross entropy $\ell_i = -\log \text{softmax}(o_i)[x_{i+1}]$. Your function should handle the following:

\begin{itemize}
    \item Subtract the largest element for numerical stability.
    \item Cancel out log and exp whenever possible.
    \item Handle any additional batch dimensions and return the average across the batch. As with section 3.3, we assume batch-like dimensions always come first, before the vocabulary size dimension.
\end{itemize}

Implement \lstinline{[adapters.run_cross_entropy]}, then run \lstinline{uv run pytest -k test_cross_entropy} to test your implementation.

\begin{answer}
The essence of implementing \lstinline{cross_entropy} is computing \lstinline{log_softmax}. We can derive the formula for \lstinline{log_softmax}. Let \lstinline{logits} be $l$, then:

\begin{equation*}
\begin{aligned}
\log \mathrm{softmax}(l_i) &= \log \frac{\exp(l_i)}{\sum_j \exp(l_j)} \\
&= l_i - \log \sum_j \exp(l_j)
\end{aligned}
\end{equation*}

Therefore, the following two computation methods are equivalent:

\begin{lstlisting}
logits = model(input)  # B,T,V
log_softmax1 = logits.log_softmax(dim=-1)  # B,T,V
log_softmax2 = logits - logits.logsumexp(dim=-1, keepdim=True)  # B,T,V
\end{lstlisting}

By using the \lstinline{logsumexp} function, we avoid explicitly computing \lstinline{exp} and \lstinline{log} operations, thereby improving numerical stability and reducing computational overhead. This approach directly computes $\log \mathrm{softmax}$, then uses \lstinline{gather} operations to obtain the log probabilities for target classes, and finally takes the negative value to get the cross-entropy loss.
\end{answer}

\begin{lstlisting}
# uv run pytest -k test_cross_entropy
def cross_entropy(inputs:torch.Tensor, targets:torch.Tensor) -> torch.Tensor:
    # (batch_size, num_classes), (batch_size,) -> scalar
    inputs = inputs - inputs.max(dim=-1, keepdim=True).values  # for numerical stability
    log_probs = inputs - torch.logsumexp(inputs, dim=-1, keepdim=True)
    return -log_probs.gather(dim=-1, index=targets.unsqueeze(-1)).squeeze(-1).mean()
\end{lstlisting}

\problem{learning\_rate\_tuning}{Tuning the learning rate (1 point)}

As we will see, one of the hyperparameters that affects training the most is the learning rate. Let's see that in practice in our toy example. Run the SGD example above with three other values for the learning rate: 1e1, 1e2, and 1e3, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?

\textbf{Deliverable}: A one-two sentence response with the behaviors you observed.

\begin{answer}
For learning rates 1e1 and 1e2, the loss decays faster (1e2 > 1e1 > 1), while for 1e3, the loss diverges, indicating that too high a learning rate can lead to instability in training. This demonstrates that while higher learning rates can accelerate convergence up to a point, there exists a critical threshold beyond which training becomes unstable and the optimization process fails.
\end{answer}

\problem{adamw}{Implement AdamW (2 points)}

\textbf{Deliverable}: Implement the AdamW optimizer as a subclass of \lstinline{torch.optim.Optimizer}. Your class should take the learning rate $\alpha$ in \lstinline{__init__}, as well as the $\beta$, $\epsilon$ and $\lambda$ hyperparameters. To help you keep state, the base \lstinline{Optimizer} class gives you a dictionary \lstinline{self.state}, which maps \lstinline{nn.Parameter} objects to a dictionary that stores any information you need for that parameter (for AdamW, this would be the moment estimates). Implement \lstinline{[adapters.get_adamw_cls]} and make sure it passes \lstinline{uv run pytest -k test_adamw}.

\begin{lstlisting}
(cs336-basics) [root:assignment1-basics]$ uv run pytest -k test_adamw
======================= test session starts ========================
platform linux -- Python 3.13.7, pytest-8.4.1, pluggy-1.6.0
rootdir: standford-cs336/assignment1-basics
configfile: pyproject.toml
plugins: jaxtyping-0.3.2, hydra-core-1.3.2
collected 48 items / 47 deselected / 1 selected                    

tests/test_optimizer.py::test_adamw PASSED

================= 1 passed, 47 deselected in 4.36s =================
\end{lstlisting}

\begin{answer}
AdamW is a variant of the Adam optimizer that decouples weight decay from the gradient-based update. The key difference is that weight decay is applied directly to the parameters rather than being added to the gradients.

The AdamW update rule is:
\begin{equation*}
\begin{aligned}
g_t &= \nabla_\theta \mathcal{L}(\theta_{t-1}) \\
\theta_t &= \theta_{t-1} - \alpha \lambda \theta_{t-1} \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= m_t / (1 - \beta_1^t) \\
\hat{v}_t &= v_t / (1 - \beta_2^t) \\
\theta_t &= \theta_{t-1} - \alpha \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon) \\
\end{aligned}
\end{equation*}

where $m_t$ and $v_t$ are the first and second moment estimates, $\lambda$ is the weight decay coefficient, and the weight decay term $\lambda \theta_{t-1}$ is applied directly to the parameters. Code is available at \href{https://github.com/donglinkang2021/cs336-assignment1-basics/blob/main/cs336_basics/optimizer.py#L30}{optimizer.py\#L30}.
\end{answer}

\problem{adamw\_accounting}{Resource accounting for training with AdamW (2 points)}

Let us compute how much memory and compute running AdamW requires. Assume we are using float32 for every tensor.

\begin{enumerate}[label=(\alph*)]
    \item How much peak memory does running AdamW require? Decompose your answer based on the memory usage of the parameters, gradients, activations, and optimizer state. Express your answer in terms of the \lstinline{batch_size} and the model hyperparameters (\lstinline{vocab_size}, \lstinline{context_length}, \lstinline{num_layers}, \lstinline{d_model}, \lstinline{num_heads}). Assume \lstinline{d_ff = 4Ã—d_model}.

    For simplicity, when calculating memory usage of activations, consider only the following components:
    \begin{itemize}
        \item Transformer block:
        \begin{itemize}
            \item RMSNorm(s)
            \item Multi-head self-attention sublayer: $QKV$ projections, $Q^T K$ matrix multiply, softmax, weighted sum of values, output projection.
            \item Position-wise feed-forward: $W_1$ matrix multiply, SiLU, $W_2$ matrix multiply
        \end{itemize}
        \item Final RMSNorm
        \item Output embedding
        \item Cross-entropy on logits
    \end{itemize}

    \textbf{Deliverable}: An algebraic expression for each of parameters, activations, gradients, and optimizer state, as well as the total.

    \begin{answer}
    \textbf{Parameters}: (See Appendix~\ref{appendix:param-verification} for verification of this formula)
    \begin{equation*}
        P = \mathop{V \cdot d}_{\text{embed}} + 
        L \cdot (
            \mathop{4d^2}_{\text{proj}} + 
            \mathop{12d^2}_{\text{ffn}} + 
            \mathop{2d}_{\text{norm}}
        ) + d + \mathop{d \cdot V}_{\text{out}} 
        = 16L d^2 + (2L + 1 + 2V)d
    \end{equation*}

    \textbf{Gradients}: $G = P$ (same as parameters)

    \textbf{Optimizer State}: $O = 2P$ (first and second moments in AdamW)

    \textbf{Activations}: 
    \begin{equation*}
    \begin{aligned}
        A =& L \cdot (
            \mathop{2B T d}_{\text{RMSNorm}} + 
            \mathop{3B T d}_{\text{QKV proj}} + 
            \mathop{B H T T}_{Q^TK} + 
            \mathop{B H T T}_{\text{softmax}} + 
            \mathop{B H T d_v}_{\text{weighted }V'} + 
            \mathop{B T d}_{\text{out proj}} + 
            \mathop{2B T d_{\text{ff}}}_{\text{W1,SiLU}} +
            \mathop{B T d}_{\text{W2}}
        ) \\
        &+ \mathop{B T d}_{\text{final RMSNorm}} + 
            \mathop{B T V}_{\text{output emb}} + 
            \mathop{B T V}_{\text{cross-entropy}} \\
        =& L \cdot (16 B T d + 2 B H T^2) + B T (d + 2 V) \\
        =& B T (L (16 d + 2 H T) + d + 2 V)
    \end{aligned}    
    \end{equation*}

    \textbf{Total Memory}: $M = P + G + O + A = 4P + A$

    (where $V$ = \lstinline{vocab_size}, $d$ = \lstinline{d_model}, $L$ = \lstinline{num_layers}, $T$ = \lstinline{context_length}, $H$ = \lstinline{num_heads}, $B$ = \lstinline{batch_size}, $d_{\text{ff}} = 4d$, and $d_v = d / H$)
    \end{answer}

    \item Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on the \lstinline{batch_size}. What is the maximum batch size you can use and still fit within 80 GB memory?

    \textbf{Deliverable}: An expression that looks like $a \cdot \text{batch\_size} + b$ for numerical values $a$, $b$, and a number representing the maximum batch size.

    \begin{answer}
    For GPT-2 XL: $V = 50257$, $d = 1600$, $L = 48$, $T = 1024$, $H = 25$

    Parameters: $P = 16 \times 48 \times 1600^2 + (2 \times 48 + 1 + 2 \times 50257) \times 1600 \approx 2.13\text{B parameters}$

    Memory expression: $M = 4P \times 4 + B \times 1024 \times (48 \times (16 \times 1600 + 2 \times 25 \times 1024) + 1600 + 2 \times 50257) \times 4$

    $M = 31.71\text{GB} + B \times 14.45\text{GB}$

    Maximum batch size: $\frac{80 - 31.71}{14.45} \approx 3.34$
    (See Appendix~\ref{appendix:memory-verification} for calculation code)
    \end{answer}

    \item How many FLOPs does running one step of AdamW take?

    \textbf{Deliverable}: An algebraic expression, with a brief justification.

    \begin{answer}
    AdamW requires approximately $16P$ FLOPs per step. The detailed breakdown is shown in Table~\ref{tab:adamw-flops}.
    
    \textbf{Assumptions:}

    a. We count only the optimizer work (gradient computation via forward+backward is not included)
    
    b. Scalar constants are precomputed once per step (e.g., $\alpha\lambda$, $(1-\beta_1^t)^{-1}$, $(1-\beta_2^t)^{-1}$)
    
    c. Each multiply/add/subtract/divide/sqrt operation counts as 1 FLOP
    
    d. $P$ is the total number of model parameters
    \end{answer}

    \begin{table}[h]
    \centering
    \caption{FLOP breakdown for AdamW optimizer per training step}
    \label{tab:adamw-flops}
    \scalebox{1}{
    \begin{tabular}{llcl}
    \toprule
    \textbf{Operation} & \textbf{Mathematical Formula} & \textbf{FLOPs/P} & \textbf{Description} \\
    \midrule
    Weight decay & $\theta \leftarrow \theta - \alpha\lambda\theta$ & 2 & 1 mul + 1 sub \\
    First moment & $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ & 3 & 2 mul + 1 add \\
    Gradient square & $g_t^2$ & 1 & 1 mul \\
    Second moment & $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ & 3 & 2 mul + 1 add \\
    Bias correction (m) & $\hat{m}_t = m_t/(1-\beta_1^t)$ & 1 & 1 mul \\
    Bias correction (v) & $\hat{v}_t = v_t/(1-\beta_2^t)$ & 1 & 1 mul \\
    Square root & $\sqrt{\hat{v}_t}$ & 1 & 1 sqrt \\
    Add epsilon & $\sqrt{\hat{v}_t} + \epsilon$ & 1 & 1 add \\
    Division & $\hat{m}_t/(\sqrt{\hat{v}_t} + \epsilon)$ & 1 & 1 div \\
    Scale by lr & $\alpha \cdot \hat{m}_t/(\sqrt{\hat{v}_t} + \epsilon)$ & 1 & 1 mul \\
    Parameter update & $\theta \leftarrow \theta - \alpha \hat{m}_t/(\sqrt{\hat{v}_t} + \epsilon)$ & 1 & 1 sub \\
    \midrule
    \multicolumn{3}{r}{\textbf{Total per parameter:}} & \textbf{16 FLOPs} \\
    \multicolumn{3}{r}{\textbf{Total for model:}} & \textbf{16P FLOPs} \\
    \bottomrule
    \end{tabular}
    }
    \end{table}

    \item Model FLOPs utilization (MFU) is defined as the ratio of observed throughput (tokens per second) relative to the hardware's theoretical peak FLOP throughput \citet{chowdhery2023palm}. An NVIDIA A100 GPU has a theoretical peak of 19.5 teraFLOP/s for float32 operations. Assuming you are able to get 50\% MFU, how long would it take to train a GPT-2 XL for 400K steps and a batch size of 1024 on a single A100? Following \citet{kaplan2020scaling} and \citet{hoffmann2022training}, assume that the backward pass has twice the FLOPs of the forward pass.

    \textbf{Deliverable}: The number of days training would take, with a brief justification.

    \begin{answer}
    \textbf{Assumptions: use our previous defined parameters for our N=2.13B GPT-2 XL:}
    
    Forward FLOPs per token: 4.51T FLOPs/1024 tokens = 4.404B FLOPs/token (approximately $2N$), 

    Backward FLOPs per token: $2 \times$ 4.404B = 8.808B FLOPs/token (approximately $4N$),

    Total FLOPs per token: 13.212B FLOPs/token (approximately $6N$)

    Total tokens: $D = 400,000 \times 1024 \times 1024 = 419,430,400,000$ tokens

    Total training FLOPs budget: $13.212B * 419.43B \approx 5.54 \times 10^{21}$ FLOPs (approximately $6ND$)

    With 50\% MFU on A100: $19.5 \times 0.5 = 9.75$ teraFLOP/s effective throughput

    Time required: $(5.54 \times 10^{21}) / (9.75 \times 10^{12}) \approx 5.68 \times 10^{8}$ seconds $\approx 157778$ hours  $\approx 6574$ days $\approx 18$ years
    \end{answer}
\end{enumerate}

\problem{learning\_rate\_schedule}{Implement cosine learning rate schedule with warmup (2 points)}

\textbf{(Warm-up)}: If $t < T_w$, then $\alpha_t = \frac{t}{T_w} \alpha_{\max}$.

\textbf{(Cosine annealing)}: If $T_w \leq t \leq T_c$, then $\alpha_t = \alpha_{\min} + \frac{1}{2}\left(1 + \cos\left(\frac{t - T_w}{T_c - T_w} \pi\right)\right)(\alpha_{\max} - \alpha_{\min})$.

\textbf{(Post-annealing)}: If $t > T_c$, then $\alpha_t = \alpha_{\min}$.

Write a function that takes $t$, $\alpha_{\max}$, $\alpha_{\min}$, $T_w$ and $T_c$, and returns the learning rate $\alpha_t$ according to the scheduler defined above. Then implement \texttt{[adapters.get\_lr\_cosine\_schedule]} and make sure it passes \texttt{uv run pytest -k test\_get\_lr\_cosine\_schedule}.

\begin{answer}
    Code is available at \href{https://github.com/donglinkang2021/cs336-assignment1-basics/blob/main/cs336_basics/optimizer.py#L183}{optimizer.py\#L183}.
\end{answer}

\begin{lstlisting}
(cs336-basics) [root:assignment1-basics]$ uv run pytest -k test_get_lr_cosine_schedule
======================== test session starts =========================
platform linux -- Python 3.13.7, pytest-8.4.1, pluggy-1.6.0
rootdir: standford-cs336/assignment1-basics
configfile: pyproject.toml
plugins: jaxtyping-0.3.2, hydra-core-1.3.2
collected 48 items / 47 deselected / 1 selected                      

tests/test_optimizer.py::test_get_lr_cosine_schedule PASSED

================== 1 passed, 47 deselected in 4.31s ==================
\end{lstlisting}

\problem{gradient\_clipping}{Implement gradient clipping (1 point)}

Write a function that implements gradient clipping. Your function should take a list of parameters and a maximum $\ell_2$-norm. It should modify each parameter gradient in place. Use $\epsilon = 10^{-6}$ (the PyTorch default). Then, implement the adapter \lstinline{[adapters.run_gradient_clipping]} and make sure it passes \lstinline{uv run pytest -k test_gradient_clipping}.

\begin{answer}
Code is available at \href{https://github.com/donglinkang2021/cs336-assignment1-basics/blob/main/cs336_basics/nn_utils.py#L27}{nn\_utils.py\#L27}.
\end{answer}

\begin{lstlisting}
(cs336-basics) [root:assignment1-basics]$ uv run pytest -k test_gradient_clipping
======================== test session starts =========================
platform linux -- Python 3.13.7, pytest-8.4.1, pluggy-1.6.0
rootdir: standford-cs336/assignment1-basics
configfile: pyproject.toml
plugins: jaxtyping-0.3.2, hydra-core-1.3.2
collected 48 items / 47 deselected / 1 selected                      

tests/test_nn_utils.py::test_gradient_clipping PASSED

================== 1 passed, 47 deselected in 2.81s ==================
\end{lstlisting}
